# [todo] add something testing codes for visualization the span
import os
import torch
import transformers
import torch.nn as nn
from transformers import BertModel

class Contriever(BertModel):
    def __init__(self, config, pooling='mean', span_pooling=None, **kwargs):
        super().__init__(config, add_pooling_layer=False)
        self.config.pooling = pooling
        self.config.span_pooling = span_pooling
        self.outputs = nn.Sequential(
                nn.Linear(self.config.hidden_size, self.config.hidden_size), 
                nn.Linear(self.config.hidden_size, 2), 
        )
        self.additional_log = {}

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        normalize=False,
        normalize_spans=False,
    ):

        model_output = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=True
        )

        last_hidden_states = model_output["last_hidden_state"]
        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)

        # sentence representation
        if self.config.pooling == 'cls':
            emb = last_hidden[:, 0]
        elif self.config.pooling == 'mean':
            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
        if normalize:
            emb = torch.nn.functional.normalize(emb, p=2, dim=-1)

        # span representation
        bsz, seq_len, hsz = last_hidden.size()
        kwargs = {'hidden': last_hidden, 'mask': attention_mask, 
                  'bsz': bsz, 'seq_len': seq_len, 'hsz': hsz,
                  'normalize': normalize_spans}

        span_emb  = self._span_extract(**kwargs)
        # if 'boundary_average' in self.config.span_pooling:
        #     span_emb, span_ids = self._boundary_average(**kwargs)
        # elif 'boundary_embedding' in self.config.span_pooling:
        #     span_emb, span_ids = self._boundary_embedding(**kwargs)
        # elif 'span_extract_average' in self.config.span_pooling:

        return emb, span_emb, None


    def _span_extract(self, hidden, mask, bsz, seq_len, **kwargs):
        # hidden = hidden[:, 1:]
        logits = self.outputs(hidden)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        # start_probs = start_logits.softmax(-1) 
        start_probs = nn.functional.gumbel_softmax(start_logits, hard=True)  
        # end_probs = end_logits.softmax(-1)
        end_probs = nn.functional.gumbel_softmax(end_logits, hard=True)  

        start_probs_vec = start_probs.cumsum(-1)
        end_probs_vec = torch.flip(torch.flip(end_probs, [0]).cumsum(-1), [0])

        span_mask = start_probs_vec * end_probs_vec # B L
        span_emb = hidden * span_mask[..., None] # B L H
        span_emb = span_emb.sum(dim=1) / span_mask.sum(dim=1)[..., None] # B L 
        self.additional_log['extract_ratio'] = span_mask.sum(dim=1).mean() / seq_len

        # original way that can not be differentiated
        # start_id = 1 + start_logits.view(bsz, -1).argmax(-1).view(-1, 1)
        # end_id = 1 + end_logits.view(bsz, -1).argmax(-1).view(-1, 1)
        # span_ids = torch.cat([start_id, end_id], dim=-1)
        # ordered = torch.arange(seq_len).repeat(bsz, 1).to(hidden.device)
        # extract_span_mask = (start_id <= ordered) & (ordered <= end_id)
        # span_emb = torch.mean(hidden * extract_span_mask.unsqueeze(-1), dim=1)

        return span_emb

    def _boundary_embedding(self, hidden, mask, bsz, seq_len, emb_size, **kwargs): 
        logits = self.outputs(hidden[:, 1:, :]) # exclude CLS 
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.softmax(1).contiguous()
        end_logits = end_logits.softmax(1).contiguous()

        start_id = start_logits.view(bsz, -1).argmax(-1) 
        end_id = end_logits.view(bsz, -1).argmax(-1)
        span_ids = torch.cat([start_id, end_id], dim=-1).view(2, -1)

        # boundary logit: p_i x p_j, shape: bsz seq_len seq_len
        # row-i means the end token, which is at the i-th of seq
        # col-j means the start token, which is at the j-th of seq
        # the trigular matrix has [:, 0] = 1, which mean j=0, i can be [0...N-1]

        # boundary logit: p_i x p_j, shape: bsz seq_len seq_len
        valid_mask = torch.ones(seq_len-1, seq_len-1).triu().T.repeat(bsz,1,1).to(hidden.device)
        boundary_logits = torch.mul(end_logits, start_logits.permute(0, 2, 1))
        boundary_logits = boundary_logits * valid_mask

        # boundary candidates: h'_ij = h_i + h_j, shape: bsz seq_len seq_len embsize
        boundary_embeddings = torch.add( 
                hidden[:, 1:, :].permute(0,2,1)[..., None],    # bsz embsize seqlen 1
                hidden[:, 1:, :].permute(0,2,1)[:, :, None, :] # bsz embsize 1 seqlen
        ).permute(0,2,3,1)  

        span_emb = (boundary_embeddings * boundary_logits[..., None]).view(bsz, -1, emb_size).sum(1)
        return span_emb, span_ids

    def _boundary_average(self, hidden, mask, bsz, emb_size, **kwargs): 
        logits = self.outputs(hidden[:, 1:, :]) # exclude CLS 
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.softmax(1).contiguous()
        end_logits = end_logits.softmax(1).contiguous()

        start_id = start_logits.view(bsz, -1).argmax(-1) 
        end_id = end_logits.view(bsz, -1).argmax(-1)
        span_ids = torch.cat([start_id, end_id], dim=-1).view(2, -1)

        failed_span_mask = torch.where(end_id > start_id, 1.0, 0.0)
        span_emb = hidden[:, 1:, :].gather(
                1, span_ids.permute(1, 0)[..., None].repeat(1,1,emb_size)
        ).mean(1)
        span_emb = span_emb * failed_span_mask.view(-1, 1) 
        return span_emb, span_ids
